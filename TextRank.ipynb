{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextRank.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "65cKHVngx2_A"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgQNKuiEBE9v",
        "outputId": "2a7975ff-3855-4fe1-b093-3efd037e2e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# Rouge score generation library\n",
        "!pip install rouge\n",
        "import rouge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "from tqdm import tqdm \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "4tocLijTD3vx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov_SAvGsHKQu",
        "outputId": "4bbfd29c-d4f2-4b07-e941-5239c5895b41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7pwbIsyCXfP",
        "outputId": "5c2192af-14f4-4898-a92e-61ffc737f301"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset manipulation"
      ],
      "metadata": {
        "id": "fiMeSuv64-c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and load data from google drive\n",
        "%%time\n",
        "\n",
        "# Local file\n",
        "filename = './wikihow_trimmed.json'\n",
        "with open(filename, 'r', encoding = 'utf-8') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "HSSCxILNBOOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a037406-b5fc-475b-9ede-cccaf867a57f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.6 s, sys: 3.33 s, total: 7.93 s\n",
            "Wall time: 12.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using textrank on the test dataset\n",
        "dataset = pd.DataFrame(data['test'])\n",
        "dataset.drop(columns = 1, inplace = True)"
      ],
      "metadata": {
        "id": "6FDdibdX8EOp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min = 1000000000\n",
        "for sample in dataset[0]:\n",
        "  if min > len(sample):\n",
        "    min = len(sample)\n",
        "print(\"Minimum sentence within the dataset:\", min)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LTo68Qj4zvZ",
        "outputId": "59a979ec-fe15-45d7-d41d-e36cdbb5d1d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum sentence within the dataset: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "NOr6NbOC5BZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReformatSample(sentences):\n",
        "  '''\n",
        "  Takes a pd dataset and converts it to a flat sentence without stop words, double white spaces, punctuation and ending white spaces\n",
        "  Input: pandas dataframe\n",
        "  Output: Flat list of sentences\n",
        "  '''\n",
        "  # Remove any punctuation, double white space, capital letters, and ending white spaces\n",
        "  sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \", regex = True)\n",
        "\n",
        "  check = False\n",
        "  while check == False:\n",
        "    sentences = pd.Series(sentences).str.replace(\"  \", \" \")\n",
        "    if any(\"  \" in sentence for sentence in sentences):\n",
        "      check = False\n",
        "    else:\n",
        "      check = True\n",
        "  sentences = [sentence.lower().strip() for sentence in sentences]\n",
        "  \n",
        "  sentences = [StripStopWords(sentence.split()) for sentence in sentences]\n",
        "\n",
        "  return sentences\n",
        "  \n",
        "def StripStopWords(sentence):\n",
        "  '''\n",
        "  Remove stop words from sentences\n",
        "  Input: List of sentences\n",
        "  Output: Corrected list of sentences\n",
        "  '''\n",
        "\n",
        "  # English stop words\n",
        "  StopWords = stopwords.words('english')\n",
        "\n",
        "  # Remove stop words\n",
        "  temp = \" \".join([i for i in sentence if i not in StopWords])\n",
        "\n",
        "  return temp"
      ],
      "metadata": {
        "id": "BlotyuRARQ4P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import glove word2vec\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "# Local file\n",
        "GloveFileName = './glove.6B.100d.txt'"
      ],
      "metadata": {
        "id": "DV1vshAp9mbN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract word vectors\n",
        "%%time \n",
        "\n",
        "WordEmbeddings = {}\n",
        "f = open(GloveFileName, encoding = 'utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coeffs = np.asarray(values[1:], dtype = 'float32')\n",
        "    WordEmbeddings[word] = coeffs\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5T5QMbz-Oae",
        "outputId": "ac59649e-6c1c-439a-9255-45822d9500be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.3 s, sys: 430 ms, total: 10.7 s\n",
            "Wall time: 11.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WordShape = WordEmbeddings['the'].shape[0]\n",
        "WordShape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXs-OJQLcel1",
        "outputId": "ed392fb6-4d1a-4cad-dead-3b383d786493"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SentenceVector(sentences, WordShape):\n",
        "  '''\n",
        "  Takes a list of sentences and retrieve the sentence vectors from each of them\n",
        "  Input: List of sentences\n",
        "  Output: Sentence vector array\n",
        "  '''\n",
        "  SentenceVecs = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "      if len(sentence) != 0:\n",
        "          WordVec = sum([WordEmbeddings.get(words, np.zeros((WordShape,))) for words in sentence.split()]) / (len(sentence.split()) + 0.0001)\n",
        "      else:\n",
        "          WordVec = np.zeros((WordShape,))\n",
        "\n",
        "      SentenceVecs.append(WordVec)\n",
        "\n",
        "  return SentenceVecs"
      ],
      "metadata": {
        "id": "0qcaQMYia5lv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SimMatrix(sentences, SentenceVector, WordShape):\n",
        "  '''\n",
        "  Create the similarity matrix of a sample given the sentences of an article along with the sentence vectors\n",
        "  Inputs: List of article sentences\n",
        "          Sentence vectors of the article sentences\n",
        "  Output: Simiarity matrix\n",
        "  '''\n",
        "  # Initialize blank similarity matrix of size\n",
        "  SimilarityMatrix = np.zeros([len(sentences), len(sentences)])\n",
        "\n",
        "  for i in range(len(sentences)):\n",
        "      for j in range(len(sentences)):\n",
        "          if i != j:\n",
        "              SimilarityMatrix[i][j] = cosine_similarity(SentenceVector[i].reshape(1, WordShape), SentenceVector[j].reshape(1, WordShape))[0, 0]\n",
        "              \n",
        "  return SimilarityMatrix"
      ],
      "metadata": {
        "id": "rmxn-T6YyO8P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GenerateSummary(sample, SummaryVariable = 10, WordShape = 100):\n",
        "  '''\n",
        "  Generate a summary of a sample article separated into a list of sentences\n",
        "  Inputs: Sample article consisting of lists of sentences\n",
        "          The fraction of the article to be summarized\n",
        "          The word shape vector\n",
        "  Output: Extractive summary of given sample article\n",
        "  '''\n",
        "  # Retrieve cleaned sentences of sample\n",
        "  sentences = ReformatSample(sample)\n",
        "\n",
        "  # Retrieve sentence vectors consisting of word vectors using gensim\n",
        "  SentenceVecs = SentenceVector(sentences, WordShape)\n",
        "\n",
        "  # Retrieve similarity matrix\n",
        "  SimilarityMatrix = SimMatrix(sentences, SentenceVecs, WordShape)\n",
        "\n",
        "  # Rank each sentence\n",
        "  NXGraph = nx.from_numpy_array(SimilarityMatrix)\n",
        "  scores = nx.pagerank_numpy(NXGraph)\n",
        "  RankedSentences = sorted(((scores[index], sentence) for index, sentence in enumerate(sentences)), reverse = True)\n",
        "\n",
        "  # Choose summarization length\n",
        "  # Ratio\n",
        "  #SummaryLength = len(sentences) // SummaryVariable\n",
        "  # Exact\n",
        "  SummaryLength = SummaryVariable\n",
        "\n",
        "  # Ensure a minimum summary length\n",
        "  # Ratio\n",
        "  #if SummaryLength == 0:\n",
        "  #  SummaryLength = 1\n",
        "  # Exact\n",
        "  if SummaryLength > len(sentences):\n",
        "    SummaryLength = len(sentences)\n",
        "\n",
        "\n",
        "  # Generate extractive summary of article\n",
        "  First = True\n",
        "\n",
        "  SummarySentence = \"\"\n",
        "  for i in range(SummaryLength):\n",
        "    if First:\n",
        "      SummarySentence += RankedSentences[i][1].capitalize()\n",
        "      First = False\n",
        "\n",
        "    else:\n",
        "      SummarySentence += \", \" + RankedSentences[i][1]\n",
        "\n",
        "  SummarySentence += \".\"\n",
        "  \n",
        "  return SummarySentence"
      ],
      "metadata": {
        "id": "v3_SfZ11Ub9y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Summaries = []\n",
        "for sample in tqdm(dataset[0]):\n",
        "  Summaries.append(GenerateSummary(sample, SummaryVariable = 10, WordShape = 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW_YIACA13mm",
        "outputId": "ff4c7503-8346-4023-e26b-8ac63c86a8bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5502/5502 [32:04<00:00,  2.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "-jpEjcCCMQ0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ground truth\n",
        "GTdataset = pd.DataFrame(data['test'])\n",
        "\n",
        "GT = []\n",
        "for _, sample in GTdataset.iterrows():\n",
        "  GTSummary = \"\"\n",
        "  #print(sample[1])\n",
        "  First = True\n",
        "  for index, i in enumerate(sample[1]):\n",
        "    if i == 1:\n",
        "      if First == True:\n",
        "        GTSummary += sample[0][index].capitalize()\n",
        "        First = False\n",
        "\n",
        "      else:\n",
        "        GTSummary += \", \" + sample[0][index]\n",
        "  GT.append(GTSummary)"
      ],
      "metadata": {
        "id": "FGYwNimwOpyd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Evaluation(index, GroundTruth, Predicted):\n",
        "  RougeScore = rouge.Rouge()\n",
        "  scores = RougeScore.get_scores(GroundTruth[index], Predicted[index], avg = True)\n",
        "\n",
        "  return scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f']\n",
        "\n",
        "def EvaluationAvg(GroundTruth, Predicted):\n",
        "  rouge1Avg, rouge2Avg, rougeLAvg = [], [], []\n",
        "  for i in range(len(GroundTruth)):\n",
        "    rouge1, rouge2, rougeL = 0, 0, 0\n",
        "    rouge1, rouge2, rougeL = Evaluation(i, GroundTruth, Predicted)\n",
        "    rouge1Avg.append(rouge1)\n",
        "    rouge2Avg.append(rouge2)\n",
        "    rougeLAvg.append(rougeL)\n",
        "    \n",
        "  print(f\"Rouge-1 Score: {round(np.mean(rouge1Avg), 2)}\")\n",
        "  print(f\"Rouge-2 Score: {round(np.mean(rouge2Avg), 2)}\")\n",
        "  print(f\"Rouge-l Score: {round(np.mean(rougeLAvg), 2)}\")\n",
        "  print(f\"Average Rouge: {round(np.mean([rouge1Avg, rouge2Avg, rougeLAvg]), 2)}\")"
      ],
      "metadata": {
        "id": "6A9yjOo2MQcq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EvaluationAvg(GT, Summaries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuYRH2IBWE1A",
        "outputId": "6116f6cd-62eb-4f2e-d387-0a92c92eb1ca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rouge-1 Score: 0.23\n",
            "Rouge-2 Score: 0.05\n",
            "Rouge-l Score: 0.22\n",
            "Average Rouge: 0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Individual sample of execution"
      ],
      "metadata": {
        "id": "65cKHVngx2_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "sentences = ReformatSample(dataset[0][1075])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzVNhv3YVRc_",
        "outputId": "a298dc8a-1521-4cd5-b4e5-20fb95415991"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17.7 ms, sys: 4.22 ms, total: 21.9 ms\n",
            "Wall time: 21.4 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orisentences = sentences"
      ],
      "metadata": {
        "id": "Ph61PQhoVcV7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SentenceVecs = SentenceVector(sentences, WordShape)"
      ],
      "metadata": {
        "id": "sJ2H5UCNeAlA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the similarity matrix\n",
        "SimilarityMatrix = np.zeros([len(orisentences), len(orisentences)])\n",
        "SimilarityMatrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJUoChppgTzr",
        "outputId": "00c6e42c-03f1-4a81-97b8-20a44a1807ed"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76, 76)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MJV9QiAGVCY",
        "outputId": "66aa2f21-b4ee-4288-9424-c2b88aa5f057"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tricky learn new pronunciation familiar letters remember additional letters becoming confident malay alphabet first step able count malay',\n",
              " 'alphabet pronunciation important role malay want learn thoroughly possible attempting master malay words',\n",
              " 'letters malay alphabet pronounced father b pronounced bay c pronounced ch chay pronounced day e pronounced elephant f pronounced fine g pronounced gold h pronounced house',\n",
              " 'pronounced ee meat j pronounced job k pronounced kitchen l pronounced life pronounced man',\n",
              " 'n pronounced nice pronounced olive p pronounced pool q pronounced kiss r pronounced rice pronounced smile pronounced time u pronounced oo mood v pronounced f free w pronounced wind',\n",
              " 'x pronounced wax pronounced year z pronounced zulu ng pronounced hanging ny prounced ana kh pronounced bach sy pronounced shield nng pronounced bingo possible speak native malay speaker simply listen talk',\n",
              " 'help detect way intonates speech general rhythm malay language',\n",
              " 'help learn speak malay much quicker looking words alone',\n",
              " 'n know native malay speaker try searching videos malay language youtube',\n",
              " 'hearing language help grasp better',\n",
              " 'searching counting malay good place begin',\n",
              " 'attempt find video features person seems comfortable language possibly introduces native speaker',\n",
              " 'note emphasis regularly falls penultimate syllable word malay',\n",
              " 'understanding verbal consistencies language help become natural speaker',\n",
              " 'understanding role numbers structure malay language first step able count malay',\n",
              " 'similar many languages malay uses cardinal ordinal numbers similar speak english cardinal numbers ones use counting ten',\n",
              " 'malay cardinal numbers convey many',\n",
              " 'numbers also known counting numbers describe quantity',\n",
              " 'malay ordinal numbers tell order things set example first second third etc',\n",
              " 'numbers show quantity instead show rank order position',\n",
              " 'order learn count ten familiarize numbers one ten able recognize sight well able pronounce correctly',\n",
              " 'next step learn spell words correctly',\n",
              " 'satu pronounced sat',\n",
              " 'dua pronounced doo uhh',\n",
              " 'tiga pronounced tee guh',\n",
              " 'empat pronounced um paht',\n",
              " 'lima pronounced lee muh',\n",
              " 'enam pronounced uhh nom',\n",
              " 'tujuh pronounced jew',\n",
              " 'lapan pronounced lah pahn',\n",
              " 'sembilan pronounced sem bee lan',\n",
              " 'sepuluh pronounced seh poo loo',\n",
              " 'learning speak native language took practice learning malay',\n",
              " 'consistent practice help master counting ten malay',\n",
              " 'use malay counting everyday objects',\n",
              " 'instead counting english native language attempt use malay everyday counting',\n",
              " 'practicing malay numbers loud help master sooner practicing head',\n",
              " 'give short quiz every days check progress learning malay numbers',\n",
              " 'confidently know numbers one ten move larger numbers',\n",
              " 'learned pronounce numbers next step use phrases sentences describe objects counting means must learn understand classifiers',\n",
              " 'classifiers words inserted numerals count nouns',\n",
              " 'classifiers allow identify type word counting equitable unit english',\n",
              " 'example dua buah rumah would mean two houses malay',\n",
              " 'counting using classifiers correct word order always number classifier noun',\n",
              " 'malay classifiers tricky learn first important part counting objects correctly',\n",
              " 'orang used count people',\n",
              " 'ekor used count animals',\n",
              " 'batang used count rod like objects',\n",
              " 'include cigarettes pens pencils',\n",
              " 'buah used count large cubical objects',\n",
              " 'include countries buildings ships vehicles furniture rooms books',\n",
              " 'biji used count spherical objects',\n",
              " 'include items cups fruits eyes',\n",
              " 'helai used count flat thin objects including paper leaves',\n",
              " 'pucuk used count firearms letters needles',\n",
              " 'bilah used count bladed objects including knives axes weapons',\n",
              " 'keping used count flat thick objects',\n",
              " 'includes objects like wooden planks',\n",
              " 'ketul used count hard objects irregular shape pebbles',\n",
              " 'bentuk used count finger rings fishing hooks',\n",
              " 'buku used count loaves bread',\n",
              " 'kuntum used count flowers individually',\n",
              " 'pintu used count shop houses terrace houses',\n",
              " 'rawan used count fishing nets',\n",
              " 'classifiers refer single several people objects refer',\n",
              " 'plurality expressed words b b rapa meaning mua meaning',\n",
              " 'replication also used express plurality example buku buku means many books becoming confident counting ability means able use classifiers numbers comfortably phrases',\n",
              " 'learned numbers one ten malay time start using phrases eventually full sentences',\n",
              " 'first learn say numbers child would learn count one two three four',\n",
              " 'next start pairing numbers vocabulary words',\n",
              " 'finally make sure memorize include correct classifier',\n",
              " 'simple phrases used practice',\n",
              " 'five dogs dua buah pulau',\n",
              " 'lapan biji pisang eight bananas lima bilah pisau',\n",
              " 'tujuh kuntum tulip seven tulips lima buku roti',\n",
              " 'five loaves bread']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(orisentences)):\n",
        "    for j in range(len(orisentences)):\n",
        "        if i != j:\n",
        "            SimilarityMatrix[i][j] = cosine_similarity(SentenceVecs[i].reshape(1,100), SentenceVecs[j].reshape(1,100))[0,0]\n",
        "\n",
        "print(SimilarityMatrix.shape)\n",
        "SimilarityMatrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkBQAvNpl5KV",
        "outputId": "4b5f775c-091a-436a-f1f5-de72f372cf2d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(76, 76)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.93819743,  0.71694201, ...,  0.02591483,\n",
              "         0.05465572,  0.41040376],\n",
              "       [ 0.93819743,  0.        ,  0.66353124, ..., -0.00560175,\n",
              "         0.04859041,  0.37325883],\n",
              "       [ 0.71694201,  0.66353124,  0.        , ...,  0.10681148,\n",
              "         0.15491981,  0.34874249],\n",
              "       ...,\n",
              "       [ 0.02591483, -0.00560175,  0.10681148, ...,  0.        ,\n",
              "         0.53405246,  0.38106482],\n",
              "       [ 0.05465572,  0.04859041,  0.15491981, ...,  0.53405246,\n",
              "         0.        ,  0.39142645],\n",
              "       [ 0.41040376,  0.37325883,  0.34874249, ...,  0.38106482,\n",
              "         0.39142645,  0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NXGraph = nx.from_numpy_array(SimilarityMatrix)\n",
        "scores = nx.pagerank_numpy(NXGraph)"
      ],
      "metadata": {
        "id": "IbTezd_wsYAo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RankedSentences = sorted(((scores[index], sentence) for index, sentence in enumerate(orisentences)), reverse = True)"
      ],
      "metadata": {
        "id": "LBD5A1ixsdZK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SummaryLength = 10\n",
        "\n",
        "# Generate summary\n",
        "First = True\n",
        "GeneratedSummary = \"\"\n",
        "for i in range(SummaryLength):\n",
        "  if First:\n",
        "    GeneratedSummary += RankedSentences[i][1].capitalize()\n",
        "    First = False\n",
        "\n",
        "  else:\n",
        "    GeneratedSummary += \", \" + RankedSentences[i][1]\n",
        "\n",
        "GeneratedSummary += \".\"\n",
        "print(GeneratedSummary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBPNPYEB0o_G",
        "outputId": "fe235886-aad5-416c-9f41-03b2958ffc51"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sepuluh pronounced seh poo loo, tiga pronounced tee guh, empat pronounced um paht, dua pronounced doo uhh, lima pronounced lee muh, sembilan pronounced sem bee lan, enam pronounced uhh nom, satu pronounced sat, tujuh pronounced jew, pronounced ee meat j pronounced job k pronounced kitchen l pronounced life pronounced man.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6vblGoIxFjl-"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}